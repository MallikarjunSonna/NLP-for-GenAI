# NLP Lesson 4: Text Representation (Turning Text into Numbers)

## ğŸ“Œ Why This Matters:
Machines donâ€™t understand words â€” they understand **numbers**.  
So we must **convert text into numerical representations** that ML or LLMs can learn from.

---

## ğŸ§  4 Ways to Represent Text

| Method                  | Works Well For           | Limitation                          |
|------------------------|--------------------------|-------------------------------------|
| 1. Bag of Words (BoW)  | Simple classification     | Ignores word order, context         |
| 2. TF-IDF              | Text importance ranking   | Still ignores context               |
| 3. Word Embeddings     | Semantic meaning (Word2Vec, GloVe) | Pre-trained, limited domain     |
| 4. Transformer Embeddings | Deep context-aware understanding | Requires GPU or cloud setup       |

---

## ğŸ”¹ 1. Bag of Words (BoW)

### ğŸ“˜ Concept:
Create a vocabulary of all unique words, then represent each sentence as a vector showing **word counts**.

### ğŸ“Œ Example:

Sent 1: "I love NLP"  
Sent 2: "NLP is fun"

### âœ… Vocabulary:
["I", "love", "NLP", "is", "fun"]

### ğŸ”¢ Vectors:

| Sentence        | Vector                |
|----------------|------------------------|
| "I love NLP"   | [1, 1, 1, 0, 0]        |
| "NLP is fun"   | [0, 0, 1, 1, 1]        |

### ğŸš« Limitation:
- No understanding of **context**
- "I love NLP" = "NLP love I"

---

## ğŸ”¹ 2. TF-IDF (Term Frequency - Inverse Document Frequency)

### ğŸ“˜ Concept:
TF = how often a word appears  
IDF = how rare that word is across documents

TF-IDF = importance of a word in a document, relative to the entire dataset

### ğŸ’¡ Use-case:
- Text classification (e.g., spam, sentiment analysis)
- Keyword extraction

### ğŸ”¢ Output:
- Gives you **weighted vectors**, not just counts

---

## ğŸ”¹ 3. Word Embeddings (Word2Vec, GloVe)

### ğŸ“˜ Concept:
Maps words into high-dimensional **dense vectors** where similar meanings are close in space.

Example:
king - man + woman â‰ˆ queen

### âœ… Use-cases:
- Text similarity
- Word analogy tasks
- Better than BoW and TF-IDF

### âš™ï¸ Tools:
- gensim for Word2Vec
- Pretrained GloVe embeddings

---

## ğŸ”¹ 4. Transformer-based Embeddings (BERT, GPT)

### ğŸ“˜ Concept:
Words are represented **in context** using the Transformer architecture.
- "Bank" in "river bank" â‰  "bank" in "money bank"
- Captures **position**, **meaning**, and **sentence-level context**

### âœ… Use-cases:
- Question answering
- Chatbots
- Document similarity
- LLM fine-tuning

### ğŸ§° Tools:
- transformers library (by Hugging Face)

---

## âœ… Summary Table

| Method        | Captures Meaning? | Captures Order? | Best For                        |
|---------------|-------------------|------------------|---------------------------------|
| Bag of Words  | âŒ No             | âŒ No            | Simple models, fast search     |
| TF-IDF        | âŒ No             | âŒ No            | Keyword importance, filtering  |
| Word2Vec      | âœ… Yes            | âŒ Partial       | Semantics, similarity          |
| BERT Embeds   | âœ… Deep Context   | âœ… Yes           | Chatbots, Q&A, GenAI models    |

---

## ğŸ§  Real-World GenAI Connection

| Use-case         | Under the Hood                    |
|------------------|-----------------------------------|
| Chatbots (GPT)   | Uses Transformer-based embeddings |
| Text summarizer  | Uses contextual embeddings        |
| Resume matcher   | Uses Word2Vec / Sentence-BERT     |
